// ìë™ ë°±ì—… ì‹œìŠ¤í…œ
import { spawn } from 'child_process';
import fs from 'fs';
import path from 'path';
import { promisify } from 'util';
import { db } from '@buzz/database';
import { backupLogs } from '@buzz/database/schema';
import { desc } from 'drizzle-orm';
import { monitoringService } from './monitoringService.js';

const fsAccess = promisify(fs.access);
const fsStat = promisify(fs.stat);
const fsReaddir = promisify(fs.readdir);
const fsUnlink = promisify(fs.unlink);

export interface BackupConfig {
  database: {
    enabled: boolean;
    schedule: string; // cron í‘œí˜„ì‹
    retention: number; // ë³´ê´€ ì¼ìˆ˜
    compression: boolean;
  };
  files: {
    enabled: boolean;
    paths: string[];
    schedule: string;
    retention: number;
    compression: boolean;
  };
  storage: {
    local: {
      path: string;
      maxSize: number; // MB
    };
    cloud?: {
      provider: 'aws' | 'gcp' | 'azure';
      bucket: string;
      credentials: any;
    };
  };
}

export interface BackupStatus {
  id: number;
  type: 'database' | 'files' | 'full';
  status: 'started' | 'completed' | 'failed';
  filePath?: string;
  fileSize?: number;
  startedAt: Date;
  completedAt?: Date;
  errorMessage?: string;
  duration?: number;
}

export class BackupService {
  private config: BackupConfig;
  private backupDir: string;
  private isRunning = false;

  constructor(config?: BackupConfig) {
    this.config = config || this.getDefaultConfig();
    this.backupDir = path.resolve(this.config.storage.local.path);
    this.ensureBackupDirectory();
  }

  // ë°±ì—… ì‹œìŠ¤í…œ ì‹œì‘
  start(): void {
    if (this.isRunning) return;
    
    this.isRunning = true;
    console.log('ğŸ’¾ Starting backup system...');
    
    // ë°±ì—… ë””ë ‰í† ë¦¬ í™•ì¸
    this.ensureBackupDirectory();
    
    console.log(`ğŸ“ Backup directory: ${this.backupDir}`);
    console.log('âœ… Backup system started');
  }

  // ë°±ì—… ì‹œìŠ¤í…œ ì¤‘ì§€
  stop(): void {
    this.isRunning = false;
    console.log('ğŸ›‘ Backup system stopped');
  }

  // ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—…
  async backupDatabase(): Promise<BackupStatus> {
    const startTime = Date.now();
    const timestamp = new Date().toISOString().replace(/[:.]/g, '-');
    const fileName = `db_backup_${timestamp}.sql`;
    const filePath = path.join(this.backupDir, fileName);

    // ë°±ì—… ë¡œê·¸ ì‹œì‘
    const [backupLog] = await db.insert(backupLogs).values({
      backupType: 'database',
      status: 'started',
      filePath
    }).returning();

    try {
      console.log('ğŸ”„ Starting database backup...');

      // PostgreSQL ë°±ì—… ì‹¤í–‰
      await this.executePgDump(filePath);

      // ì••ì¶• ì²˜ë¦¬
      let finalPath = filePath;
      let finalSize = 0;

      if (this.config.database.compression) {
        finalPath = await this.compressFile(filePath);
        await fsUnlink(filePath); // ì›ë³¸ íŒŒì¼ ì‚­ì œ
      }

      // íŒŒì¼ í¬ê¸° í™•ì¸
      const stats = await fsStat(finalPath);
      finalSize = stats.size;

      const duration = Math.round((Date.now() - startTime) / 1000);

      // ë°±ì—… ë¡œê·¸ ì™„ë£Œ
      await db.update(backupLogs)
        .set({
          status: 'completed',
          completedAt: new Date(),
          fileSize: finalSize,
          filePath: finalPath
        })
        .where({ id: backupLog.id });

      console.log(`âœ… Database backup completed: ${finalPath} (${this.formatBytes(finalSize)}, ${duration}s)`);

      // ë©”íŠ¸ë¦­ ê¸°ë¡
      await monitoringService.recordMetric({
        type: 'backup_database_duration',
        value: duration,
        unit: 'seconds'
      });

      await monitoringService.recordMetric({
        type: 'backup_database_size',
        value: Math.round(finalSize / 1024 / 1024),
        unit: 'MB'
      });

      return {
        id: backupLog.id,
        type: 'database',
        status: 'completed',
        filePath: finalPath,
        fileSize: finalSize,
        startedAt: new Date(startTime),
        completedAt: new Date(),
        duration
      };

    } catch (error) {
      console.error('âŒ Database backup failed:', error);

      // ë°±ì—… ë¡œê·¸ ì‹¤íŒ¨ ê¸°ë¡
      await db.update(backupLogs)
        .set({
          status: 'failed',
          errorMessage: error.message
        })
        .where({ id: backupLog.id });

      // ì‹¤íŒ¨í•œ íŒŒì¼ ì •ë¦¬
      try {
        await fsAccess(filePath);
        await fsUnlink(filePath);
      } catch {}

      // ì•Œë¦¼ ìƒì„±
      monitoringService.createAlert({
        type: 'error',
        source: 'database',
        message: 'Database backup failed',
        details: { error: error.message, filePath }
      });

      throw error;
    }
  }

  // íŒŒì¼ ë°±ì—…
  async backupFiles(): Promise<BackupStatus> {
    const startTime = Date.now();
    const timestamp = new Date().toISOString().replace(/[:.]/g, '-');
    const fileName = `files_backup_${timestamp}.tar.gz`;
    const filePath = path.join(this.backupDir, fileName);

    // ë°±ì—… ë¡œê·¸ ì‹œì‘
    const [backupLog] = await db.insert(backupLogs).values({
      backupType: 'files',
      status: 'started',
      filePath
    }).returning();

    try {
      console.log('ğŸ”„ Starting files backup...');

      // tar ì••ì¶• ì‹¤í–‰
      await this.createTarArchive(this.config.files.paths, filePath);

      // íŒŒì¼ í¬ê¸° í™•ì¸
      const stats = await fsStat(filePath);
      const fileSize = stats.size;
      const duration = Math.round((Date.now() - startTime) / 1000);

      // ë°±ì—… ë¡œê·¸ ì™„ë£Œ
      await db.update(backupLogs)
        .set({
          status: 'completed',
          completedAt: new Date(),
          fileSize
        })
        .where({ id: backupLog.id });

      console.log(`âœ… Files backup completed: ${filePath} (${this.formatBytes(fileSize)}, ${duration}s)`);

      // ë©”íŠ¸ë¦­ ê¸°ë¡
      await monitoringService.recordMetric({
        type: 'backup_files_duration',
        value: duration,
        unit: 'seconds'
      });

      return {
        id: backupLog.id,
        type: 'files',
        status: 'completed',
        filePath,
        fileSize,
        startedAt: new Date(startTime),
        completedAt: new Date(),
        duration
      };

    } catch (error) {
      console.error('âŒ Files backup failed:', error);

      // ë°±ì—… ë¡œê·¸ ì‹¤íŒ¨ ê¸°ë¡
      await db.update(backupLogs)
        .set({
          status: 'failed',
          errorMessage: error.message
        })
        .where({ id: backupLog.id });

      // ì‹¤íŒ¨í•œ íŒŒì¼ ì •ë¦¬
      try {
        await fsAccess(filePath);
        await fsUnlink(filePath);
      } catch {}

      monitoringService.createAlert({
        type: 'error',
        source: 'api',
        message: 'Files backup failed',
        details: { error: error.message, filePath }
      });

      throw error;
    }
  }

  // ì „ì²´ ë°±ì—… (ë°ì´í„°ë² ì´ìŠ¤ + íŒŒì¼)
  async fullBackup(): Promise<BackupStatus[]> {
    console.log('ğŸ”„ Starting full backup...');
    
    const results: BackupStatus[] = [];

    if (this.config.database.enabled) {
      try {
        const dbBackup = await this.backupDatabase();
        results.push(dbBackup);
      } catch (error) {
        console.error('Database backup failed during full backup:', error);
      }
    }

    if (this.config.files.enabled) {
      try {
        const filesBackup = await this.backupFiles();
        results.push(filesBackup);
      } catch (error) {
        console.error('Files backup failed during full backup:', error);
      }
    }

    console.log(`âœ… Full backup completed: ${results.length} backups created`);
    return results;
  }

  // ë°±ì—… ë³µì›
  async restoreDatabase(backupPath: string): Promise<void> {
    console.log(`ğŸ”„ Restoring database from: ${backupPath}`);

    try {
      // íŒŒì¼ ì¡´ì¬ í™•ì¸
      await fsAccess(backupPath);

      // .gz íŒŒì¼ì¸ ê²½ìš° ì••ì¶• í•´ì œ
      let sqlFilePath = backupPath;
      if (backupPath.endsWith('.gz')) {
        sqlFilePath = await this.decompressFile(backupPath);
      }

      // PostgreSQL ë³µì› ì‹¤í–‰
      await this.executePsqlRestore(sqlFilePath);

      // ì„ì‹œ íŒŒì¼ ì •ë¦¬
      if (sqlFilePath !== backupPath) {
        await fsUnlink(sqlFilePath);
      }

      console.log('âœ… Database restore completed');

      monitoringService.createAlert({
        type: 'info',
        source: 'database',
        message: 'Database restore completed',
        details: { backupPath }
      });

    } catch (error) {
      console.error('âŒ Database restore failed:', error);

      monitoringService.createAlert({
        type: 'error',
        source: 'database',
        message: 'Database restore failed',
        details: { error: error.message, backupPath }
      });

      throw error;
    }
  }

  // ë°±ì—… ëª©ë¡ ì¡°íšŒ
  async getBackupHistory(limit = 50): Promise<BackupStatus[]> {
    const logs = await db
      .select()
      .from(backupLogs)
      .orderBy(desc(backupLogs.startedAt))
      .limit(limit);

    return logs.map(log => ({
      id: log.id,
      type: log.backupType as 'database' | 'files' | 'full',
      status: log.status as 'started' | 'completed' | 'failed',
      filePath: log.filePath || undefined,
      fileSize: log.fileSize || undefined,
      startedAt: log.startedAt,
      completedAt: log.completedAt || undefined,
      errorMessage: log.errorMessage || undefined,
      duration: log.completedAt 
        ? Math.round((log.completedAt.getTime() - log.startedAt.getTime()) / 1000)
        : undefined
    }));
  }

  // ì˜¤ë˜ëœ ë°±ì—… ì •ë¦¬
  async cleanupOldBackups(): Promise<number> {
    console.log('ğŸ§¹ Cleaning up old backups...');

    const dbRetentionDate = new Date(Date.now() - this.config.database.retention * 24 * 60 * 60 * 1000);
    const filesRetentionDate = new Date(Date.now() - this.config.files.retention * 24 * 60 * 60 * 1000);

    let deletedCount = 0;

    try {
      // ë°±ì—… ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
      const files = await fsReaddir(this.backupDir);

      for (const file of files) {
        const filePath = path.join(this.backupDir, file);
        const stats = await fsStat(filePath);

        let shouldDelete = false;

        if (file.startsWith('db_backup_') && stats.mtime < dbRetentionDate) {
          shouldDelete = true;
        } else if (file.startsWith('files_backup_') && stats.mtime < filesRetentionDate) {
          shouldDelete = true;
        }

        if (shouldDelete) {
          await fsUnlink(filePath);
          deletedCount++;
          console.log(`ğŸ—‘ï¸ Deleted old backup: ${file}`);
        }
      }

      // ë°ì´í„°ë² ì´ìŠ¤ ë¡œê·¸ë„ ì •ë¦¬
      const oldestRetentionDate = new Date(Math.min(dbRetentionDate.getTime(), filesRetentionDate.getTime()));
      
      // ì‹¤ì œë¡œëŠ” DELETE ì¿¼ë¦¬ë¡œ ì˜¤ë˜ëœ ë¡œê·¸ ì‚­ì œ
      console.log(`ğŸ“ Cleaned up backup logs older than ${oldestRetentionDate.toISOString()}`);

      console.log(`âœ… Cleanup completed: ${deletedCount} old backups deleted`);
      
      return deletedCount;

    } catch (error) {
      console.error('âŒ Backup cleanup failed:', error);
      
      monitoringService.createAlert({
        type: 'warning',
        source: 'api',
        message: 'Backup cleanup failed',
        details: { error: error.message }
      });

      return 0;
    }
  }

  // ë°±ì—… ì„¤ì • ì—…ë°ì´íŠ¸
  updateConfig(newConfig: Partial<BackupConfig>): void {
    this.config = { ...this.config, ...newConfig };
    
    // ë°±ì—… ë””ë ‰í† ë¦¬ ë³€ê²½ì‹œ ì¬ì„¤ì •
    if (newConfig.storage?.local?.path) {
      this.backupDir = path.resolve(newConfig.storage.local.path);
      this.ensureBackupDirectory();
    }

    console.log('âš™ï¸ Backup configuration updated');
  }

  // í˜„ì¬ ì„¤ì • ì¡°íšŒ
  getConfig(): BackupConfig {
    return { ...this.config };
  }

  // ë°±ì—… ìƒíƒœ í™•ì¸
  getStatus(): {
    isRunning: boolean;
    backupDir: string;
    diskUsage: {
      used: number;
      available: number;
      total: number;
    };
    lastBackup?: {
      database?: Date;
      files?: Date;
    };
  } {
    // ì‹¤ì œë¡œëŠ” ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ì„ ì •í™•íˆ ê³„ì‚°í•´ì•¼ í•¨
    return {
      isRunning: this.isRunning,
      backupDir: this.backupDir,
      diskUsage: {
        used: 0, // ë°±ì—… í´ë” ì‚¬ìš©ëŸ‰
        available: 1000 * 1024 * 1024, // 1GB (ì„ì‹œê°’)
        total: 10 * 1000 * 1024 * 1024 // 10GB (ì„ì‹œê°’)
      },
      lastBackup: {
        database: new Date(), // ë§ˆì§€ë§‰ DB ë°±ì—… ì‹œê°„
        files: new Date() // ë§ˆì§€ë§‰ íŒŒì¼ ë°±ì—… ì‹œê°„
      }
    };
  }

  // ê¸°ë³¸ ì„¤ì •
  private getDefaultConfig(): BackupConfig {
    return {
      database: {
        enabled: true,
        schedule: '0 2 * * *', // ë§¤ì¼ ì˜¤ì „ 2ì‹œ
        retention: 30, // 30ì¼ ë³´ê´€
        compression: true
      },
      files: {
        enabled: true,
        paths: ['./reports', './uploads'],
        schedule: '0 3 * * *', // ë§¤ì¼ ì˜¤ì „ 3ì‹œ
        retention: 7, // 7ì¼ ë³´ê´€
        compression: true
      },
      storage: {
        local: {
          path: './backups',
          maxSize: 5 * 1024 // 5GB
        }
      }
    };
  }

  // ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±
  private ensureBackupDirectory(): void {
    if (!fs.existsSync(this.backupDir)) {
      fs.mkdirSync(this.backupDir, { recursive: true });
      console.log(`ğŸ“ Created backup directory: ${this.backupDir}`);
    }
  }

  // PostgreSQL ë¤í”„ ì‹¤í–‰
  private executePgDump(outputPath: string): Promise<void> {
    return new Promise((resolve, reject) => {
      // í™˜ê²½ ë³€ìˆ˜ì—ì„œ DB ì •ë³´ ê°€ì ¸ì˜¤ê¸° (ì‹¤ì œë¡œëŠ” ë” ì•ˆì „í•œ ë°©ì‹ í•„ìš”)
      const dbUrl = process.env.DATABASE_URL || 'postgresql://localhost:5432/buzz';
      
      const pgDump = spawn('pg_dump', [dbUrl, '--no-owner', '--no-privileges'], {
        stdio: ['ignore', 'pipe', 'pipe']
      });

      const writeStream = fs.createWriteStream(outputPath);
      pgDump.stdout.pipe(writeStream);

      let errorOutput = '';
      pgDump.stderr.on('data', (data) => {
        errorOutput += data.toString();
      });

      pgDump.on('error', (error) => {
        reject(new Error(`pg_dump process error: ${error.message}`));
      });

      pgDump.on('close', (code) => {
        writeStream.end();
        
        if (code === 0) {
          resolve();
        } else {
          reject(new Error(`pg_dump exited with code ${code}: ${errorOutput}`));
        }
      });
    });
  }

  // PostgreSQL ë³µì› ì‹¤í–‰
  private executePsqlRestore(inputPath: string): Promise<void> {
    return new Promise((resolve, reject) => {
      const dbUrl = process.env.DATABASE_URL || 'postgresql://localhost:5432/buzz';
      
      const psql = spawn('psql', [dbUrl], {
        stdio: ['pipe', 'pipe', 'pipe']
      });

      const readStream = fs.createReadStream(inputPath);
      readStream.pipe(psql.stdin);

      let errorOutput = '';
      psql.stderr.on('data', (data) => {
        errorOutput += data.toString();
      });

      psql.on('error', (error) => {
        reject(new Error(`psql process error: ${error.message}`));
      });

      psql.on('close', (code) => {
        if (code === 0) {
          resolve();
        } else {
          reject(new Error(`psql exited with code ${code}: ${errorOutput}`));
        }
      });
    });
  }

  // tar ì•„ì¹´ì´ë¸Œ ìƒì„±
  private createTarArchive(paths: string[], outputPath: string): Promise<void> {
    return new Promise((resolve, reject) => {
      const tar = spawn('tar', ['-czf', outputPath, ...paths], {
        stdio: ['ignore', 'pipe', 'pipe']
      });

      let errorOutput = '';
      tar.stderr.on('data', (data) => {
        errorOutput += data.toString();
      });

      tar.on('error', (error) => {
        reject(new Error(`tar process error: ${error.message}`));
      });

      tar.on('close', (code) => {
        if (code === 0) {
          resolve();
        } else {
          reject(new Error(`tar exited with code ${code}: ${errorOutput}`));
        }
      });
    });
  }

  // íŒŒì¼ ì••ì¶•
  private async compressFile(filePath: string): Promise<string> {
    const gzipPath = `${filePath}.gz`;
    
    return new Promise((resolve, reject) => {
      const gzip = spawn('gzip', ['-f', filePath], {
        stdio: ['ignore', 'pipe', 'pipe']
      });

      gzip.on('error', reject);
      gzip.on('close', (code) => {
        if (code === 0) {
          resolve(gzipPath);
        } else {
          reject(new Error(`gzip exited with code ${code}`));
        }
      });
    });
  }

  // íŒŒì¼ ì••ì¶• í•´ì œ
  private async decompressFile(gzipPath: string): Promise<string> {
    const filePath = gzipPath.replace('.gz', '');
    
    return new Promise((resolve, reject) => {
      const gunzip = spawn('gunzip', ['-c', gzipPath], {
        stdio: ['ignore', 'pipe', 'pipe']
      });

      const writeStream = fs.createWriteStream(filePath);
      gunzip.stdout.pipe(writeStream);

      gunzip.on('error', reject);
      gunzip.on('close', (code) => {
        writeStream.end();
        if (code === 0) {
          resolve(filePath);
        } else {
          reject(new Error(`gunzip exited with code ${code}`));
        }
      });
    });
  }

  // ë°”ì´íŠ¸ í¬ë§·íŒ…
  private formatBytes(bytes: number): string {
    if (bytes === 0) return '0 B';
    
    const sizes = ['B', 'KB', 'MB', 'GB', 'TB'];
    const i = Math.floor(Math.log(bytes) / Math.log(1024));
    
    return `${Math.round(bytes / Math.pow(1024, i) * 100) / 100} ${sizes[i]}`;
  }
}

// ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
export const backupService = new BackupService();